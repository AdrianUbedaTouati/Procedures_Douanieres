# ğŸ”„ Flujo de EjecuciÃ³n del Chat - TenderAI v3.7

**Sistema Function Calling con Review Loop AutomÃ¡tico**

---

## ğŸ“‹ Ãndice

1. [VisiÃ³n General](#visiÃ³n-general)
2. [Flujo Completo Paso a Paso](#flujo-completo-paso-a-paso)
3. [Review Loop en Detalle](#review-loop-en-detalle)
4. [Ejemplos Reales](#ejemplos-reales)

---

## VisiÃ³n General

```
Usuario â†’ Django â†’ ChatAgentService â†’ Agent (Iter 1) â†’ ResponseReviewer â†’ Agent (Iter 2) â†’ Respuesta Final
                                            â†“                                    â†“
                                      Tools (16)                            Tools (16)
```

**Novedades v3.7:**
- âœ… Review Loop SIEMPRE ejecutado
- âœ… Segunda iteraciÃ³n automÃ¡tica con feedback
- âœ… Merge de resultados de ambas iteraciones
- âœ… Metadata de review tracking

**Llamadas mÃ­nimas:**
- 2 iteraciones de agent (inicial + mejorada)
- 1 llamada de review
- Total: 3+ llamadas LLM por mensaje

---

## Flujo Completo Paso a Paso

### ğŸ¯ PASO 1: Usuario EnvÃ­a Mensaje

**Archivo:** `chat/views.py` â†’ `ChatMessageCreateView.post()`

```python
user_message_content = request.POST.get('message', '').strip()

# Crear mensaje en BD
user_message = ChatMessage.objects.create(
    session=session,
    role='user',
    content=user_message_content
)
```

**Logs:**
```
[CHAT REQUEST] Usuario: usuario@ejemplo.com (OPENAI)
[CHAT REQUEST] SesiÃ³n ID: 42
[CHAT REQUEST] Mensaje: Dame las mejores licitaciones de software
```

---

### ğŸ¯ PASO 2: Preparar Historial

**Archivo:** `chat/views.py`

```python
# Obtener mensajes anteriores
previous_messages = session.messages.filter(
    created_at__lt=user_message.created_at
).order_by('created_at')

# Convertir a formato estÃ¡ndar
conversation_history = [
    {'role': msg.role, 'content': msg.content}
    for msg in previous_messages
]
```

---

### ğŸ¯ PASO 3: ChatAgentService - ITERACIÃ“N INICIAL

**Archivo:** `chat/services.py` â†’ `process_message()`

```python
# Crear agente
agent = FunctionCallingAgent(
    llm_provider=user.llm_provider,
    llm_model=user.openai_model,
    llm_api_key=user.llm_api_key,
    retriever=retriever,
    db_session=None,
    user=user
)

# ITERACIÃ“N 1: Ejecutar query inicial
result = agent.query(message, conversation_history)
response_content = result['answer']
```

**Logs:**
```
[SERVICE] Ejecutando query en el agente...
[SERVICE] Mensaje: Dame las mejores licitaciones de software
```

---

### ğŸ¯ PASO 4: FunctionCallingAgent - ITERACIÃ“N 1

**Archivo:** `agent_ia_core/agent_function_calling.py`

**Loop de Function Calling:**
```python
for iteration in range(1, max_iterations + 1):  # max 15
    # 1. LLM decide tools
    response = self._call_llm_with_tools(messages)
    tool_calls = response.get('tool_calls', [])

    if not tool_calls:
        # Respuesta final
        break

    # 2. Ejecutar tools
    results = self.tool_registry.execute_tool_calls(tool_calls)

    # 3. AÃ±adir resultados a mensajes
    # 4. Continuar loop
```

**Ejemplo ejecuciÃ³n:**
```
IteraciÃ³n 1:
  LLM decide: search_tenders(query="software", limit=10)
  Tool ejecuta: 10 licitaciones encontradas

IteraciÃ³n 2:
  LLM decide: get_company_info()
  Tool ejecuta: Perfil de empresa obtenido

IteraciÃ³n 3:
  LLM genera respuesta final con ambos datos
  No tool_calls â†’ FIN
```

**Resultado:**
```python
{
    'answer': "He encontrado 10 licitaciones de software...",
    'documents': [doc1, doc2, ...],
    'tools_used': ['search_tenders', 'get_company_info'],
    'iterations': 3
}
```

---

### ğŸ¯ PASO 5: ResponseReviewer - REVISIÃ“N â­ NUEVO

**Archivo:** `chat/services.py` â†’ `process_message()`

```python
# REVIEW LOOP (SIEMPRE ejecutado)
from chat.response_reviewer import ResponseReviewer

reviewer = ResponseReviewer(agent.llm)

review_result = reviewer.review_response(
    user_question=message,
    conversation_history=formatted_history,
    initial_response=response_content,
    metadata={
        'documents_used': result.get('documents', []),
        'tools_used': result.get('tools_used', []),
        'route': result.get('route', 'unknown')
    }
)
```

**Logs:**
```
[SERVICE] Iniciando revisiÃ³n de respuesta...
[REVIEWER] Llamando al LLM revisor...
[REVIEWER] RevisiÃ³n completada: NEEDS_IMPROVEMENT (score: 75/100)
```

**ResponseReviewer analiza:**

1. **FORMATO (30%):**
   - Â¿Usa Markdown correctamente?
   - Â¿Headers ## para cada licitaciÃ³n?
   - Â¿Estructura clara?

2. **CONTENIDO (40%):**
   - Â¿Responde completamente?
   - Â¿Incluye presupuestos, plazos?
   - Â¿Falta informaciÃ³n?

3. **ANÃLISIS (30%):**
   - Â¿Justifica por quÃ© son las "mejores"?
   - Â¿Usa datos objetivos?
   - Â¿Es Ãºtil?

**Resultado del review:**
```python
{
    'status': 'NEEDS_IMPROVEMENT',  # o 'APPROVED'
    'score': 75,
    'issues': [
        'Falta justificaciÃ³n de por quÃ© son las mejores',
        'No incluye anÃ¡lisis de fit con perfil de empresa'
    ],
    'suggestions': [
        'Agregar anÃ¡lisis de match con experiencia del usuario',
        'Incluir presupuestos y plazos de cada licitaciÃ³n'
    ],
    'feedback': 'La respuesta lista las licitaciones pero no explica por quÃ© son las mejores para el usuario. Falta anÃ¡lisis personalizado.'
}
```

---

### ğŸ¯ PASO 6: Segunda IteraciÃ³n - MEJORA â­ SIEMPRE

**Archivo:** `chat/services.py` â†’ `process_message()`

```python
# SIEMPRE ejecutar segunda iteraciÃ³n
print("[SERVICE] Ejecutando segunda iteraciÃ³n de mejora (siempre activo)...")

# Construir prompt de mejora
issues_list = '\n'.join([f"- {issue}" for issue in review_result['issues']])
suggestions_list = '\n'.join([f"- {sug}" for sug in review_result['suggestions']])

improvement_prompt = f"""Tu respuesta anterior fue revisada. Vamos a mejorarla.

**Tu respuesta original:**
{response_content}

**Problemas detectados:**
{issues_list if issues_list else '- NingÃºn problema grave detectado'}

**Sugerencias:**
{suggestions_list if suggestions_list else '- Mantener el buen formato actual'}

**Feedback del revisor:**
{review_result['feedback'] if review_result['feedback'] else 'La respuesta estÃ¡ bien estructurada, pero siempre podemos mejorarla.'}

**Tu tarea:**
Genera una respuesta MEJORADA que sea aÃºn mÃ¡s completa y Ãºtil.

**IMPORTANTE:**
- Usa herramientas (tools) si necesitas buscar mÃ¡s informaciÃ³n
- Si faltan datos especÃ­ficos (presupuestos, plazos, etc.), bÃºscalos
- Si el formato es incorrecto, corrÃ­gelo (usa ## para licitaciones mÃºltiples)
- Si falta anÃ¡lisis, justifica tus recomendaciones con datos concretos

**Pregunta original del usuario:**
{message}

Genera tu respuesta mejorada:"""
```

**Ejecutar con historial extendido:**
```python
improvement_history = formatted_history + [
    {'role': 'user', 'content': message},
    {'role': 'assistant', 'content': response_content}
]

improved_result = agent.query(
    improvement_prompt,
    conversation_history=improvement_history
)
```

**Logs:**
```
[SERVICE] Ejecutando segunda iteraciÃ³n de mejora (siempre activo)...
[SERVICE] Ejecutando query de mejora...
```

**El agente en la 2da iteraciÃ³n:**
```
IteraciÃ³n 1:
  LLM lee feedback
  LLM decide: get_tender_details(tender_id="00668461-2025")
            get_tender_details(tender_id="00677736-2025")
  Tools ejecutan: Detalles completos de 2 licitaciones

IteraciÃ³n 2:
  LLM genera respuesta MEJORADA con:
    - AnÃ¡lisis personalizado basado en perfil
    - Presupuestos y plazos especÃ­ficos
    - JustificaciÃ³n de por quÃ© cada una es adecuada
    - Formato correcto con ##
  No tool_calls â†’ FIN
```

**Resultado mejorado:**
```python
{
    'answer': """BasÃ¡ndome en tu perfil de empresa de desarrollo de software, te recomiendo:

## Desarrollo de plataforma ERP - ID: 00668461-2025

**Por quÃ© es la mÃ¡s adecuada:**
- Presupuesto: 961,200 EUR - Ideal para empresas de tu tamaÃ±o
- Tu experiencia en desarrollo coincide con el CPV 72267100
- Plazo: 45 dÃ­as restantes, tiempo suficiente para preparar propuesta

**AnÃ¡lisis de fit:**
- Match 95% con tu perfil
- Sector: Desarrollo de software (tu especialidad)
- Presupuesto adecuado para tu capacidad

## Sistema de gestiÃ³n documental - ID: 00677736-2025

**Por quÃ© es recomendable:**
- Presupuesto: 750,000 EUR
- Plazo: 30 dÃ­as restantes
- Match 90% con tu experiencia

**Datos clave:**
- CPV: 72000000 (Software)
- UbicaciÃ³n: ES300 (Madrid)
- Comprador: Autoridad Portuaria

[ID: 00668461-2025 | title]
[ID: 00677736-2025 | title]""",
    'documents': [doc1, doc2, doc3, doc4],  # Nuevos docs
    'tools_used': ['get_tender_details'],  # Nuevas tools
    'iterations': 2
}
```

---

### ğŸ¯ PASO 7: Merge de Resultados

**Archivo:** `chat/services.py`

```python
# Update response con versiÃ³n mejorada
response_content = improved_result.get('answer', response_content)

# Merge documents (evitar duplicados)
existing_doc_ids = {doc.get('ojs_notice_id') for doc in result.get('documents', [])}
new_docs = [
    doc for doc in improved_result.get('documents', [])
    if doc.get('ojs_notice_id') not in existing_doc_ids
]
result['documents'] = result.get('documents', []) + new_docs

# Merge tools used
result['tools_used'] = list(set(
    result.get('tools_used', []) + improved_result.get('tools_used', [])
))

# Update iterations count
result['iterations'] = result.get('iterations', 0) + improved_result.get('iterations', 0)
```

**Resultado final combinado:**
```python
{
    'answer': "[respuesta mejorada completa]",
    'documents': [doc1, doc2, doc3, doc4],  # Iter 1 + Iter 2
    'tools_used': ['search_tenders', 'get_company_info', 'get_tender_details'],
    'iterations': 5,  # 3 de iter1 + 2 de iter2
    'review': {
        'review_performed': True,
        'review_status': 'NEEDS_IMPROVEMENT',
        'review_score': 75,
        'review_issues': ['...'],
        'review_suggestions': ['...'],
        'improvement_applied': True
    }
}
```

---

### ğŸ¯ PASO 8: Guardar en BD

**Archivo:** `chat/views.py`

```python
assistant_message = ChatMessage.objects.create(
    session=session,
    role='assistant',
    content=response['content'],  # Respuesta mejorada
    metadata={
        'route': response['metadata'].get('route'),
        'num_documents': len(documents_used),
        'tools_used': response['metadata'].get('tools_used'),
        'iterations': response['metadata'].get('iterations'),
        'total_tokens': cost_data['total_tokens'],
        'cost_eur': cost_data['total_cost_eur'],
        # Review tracking
        'review': response['metadata'].get('review')
    }
)
```

**Logs:**
```
[SERVICE] âœ“ Respuesta mejorada generada: 850 caracteres
[SERVICE] âœ“ Respuesta procesada: 850 caracteres
[SERVICE] Documentos recuperados: 4
[SERVICE] Herramientas usadas: search_tenders â†’ get_company_info â†’ get_tender_details
[SERVICE] Tokens totales: 1250 (in: 600, out: 650)
[SERVICE] Review - Status: NEEDS_IMPROVEMENT, Score: 75/100
[SERVICE] Review - Mejora aplicada (2da iteraciÃ³n ejecutada)
```

---

### ğŸ¯ PASO 9: Respuesta al Frontend

**Archivo:** `chat/views.py`

```python
return JsonResponse({
    'success': True,
    'message': {
        'id': assistant_message.id,
        'content': assistant_message.content,
        'created_at': assistant_message.created_at.isoformat(),
        'role': 'assistant',
        'metadata': assistant_message.metadata
    }
})
```

**JSON enviado:**
```json
{
  "success": true,
  "message": {
    "id": 1234,
    "content": "BasÃ¡ndome en tu perfil de empresa...",
    "role": "assistant",
    "metadata": {
      "route": "function_calling",
      "num_documents": 4,
      "tools_used": ["search_tenders", "get_company_info", "get_tender_details"],
      "iterations": 5,
      "total_tokens": 1250,
      "cost_eur": 0.0125,
      "review": {
        "review_performed": true,
        "review_status": "NEEDS_IMPROVEMENT",
        "review_score": 75,
        "review_issues": ["Falta justificaciÃ³n..."],
        "review_suggestions": ["Agregar anÃ¡lisis..."],
        "improvement_applied": true
      }
    }
  }
}
```

---

## ğŸ”„ Review Loop en Detalle

### Arquitectura del Review

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ITERACIÃ“N INICIAL                             â”‚
â”‚  Agent ejecuta tools â†’ Genera respuesta â†’ Retorna resultado     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    RESPONSEREVIEWER                              â”‚
â”‚                                                                  â”‚
â”‚  Input:                                                          â”‚
â”‚  - user_question: "Dame las mejores licitaciones de software"  â”‚
â”‚  - conversation_history: [...]                                  â”‚
â”‚  - initial_response: "He encontrado 10 licitaciones..."        â”‚
â”‚  - metadata: {documents, tools_used, route}                     â”‚
â”‚                                                                  â”‚
â”‚  Proceso:                                                        â”‚
â”‚  1. Construir prompt de revisiÃ³n con criterios                 â”‚
â”‚  2. Llamar LLM con prompt                                       â”‚
â”‚  3. Parsear respuesta (STATUS, SCORE, ISSUES, SUGGESTIONS)     â”‚
â”‚  4. Retornar anÃ¡lisis estructurado                              â”‚
â”‚                                                                  â”‚
â”‚  Output:                                                         â”‚
â”‚  {                                                               â”‚
â”‚    status: 'NEEDS_IMPROVEMENT',                                 â”‚
â”‚    score: 75,                                                    â”‚
â”‚    issues: [...],                                                â”‚
â”‚    suggestions: [...],                                           â”‚
â”‚    feedback: "..."                                               â”‚
â”‚  }                                                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚               SEGUNDA ITERACIÃ“N (SIEMPRE)                        â”‚
â”‚                                                                  â”‚
â”‚  Prompt mejorado:                                                â”‚
â”‚  "Tu respuesta: [...]                                            â”‚
â”‚   Problemas: [...]                                               â”‚
â”‚   Sugerencias: [...]                                             â”‚
â”‚   Feedback: [...]                                                â”‚
â”‚                                                                  â”‚
â”‚   Genera respuesta MEJORADA con acceso a tools"                â”‚
â”‚                                                                  â”‚
â”‚  Agent ejecuta con tools completos â†’ Respuesta mejorada         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    MERGE Y RETORNO                               â”‚
â”‚  - Response final = respuesta mejorada                           â”‚
â”‚  - Documents = iter1 + iter2                                     â”‚
â”‚  - Tools = union de ambas                                        â”‚
â”‚  - Metadata incluye review tracking                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Criterios de EvaluaciÃ³n

**Prompt del ResponseReviewer:**
```
Analiza la respuesta y evalÃºa:

1. FORMATO (30 puntos):
   - Â¿Usa Markdown correctamente?
   - Si hay mÃºltiples licitaciones, Â¿usa ## para cada una?
   - Â¿EstÃ¡ bien estructurado y legible?

2. CONTENIDO (40 puntos):
   - Â¿Responde completamente a la pregunta?
   - Â¿Incluye todos los datos relevantes (IDs, presupuestos, plazos)?
   - Â¿Falta informaciÃ³n importante?

3. ANÃLISIS (30 puntos):
   - Si pidiÃ³ recomendaciones, Â¿justifica con datos?
   - Â¿Usa los documentos consultados correctamente?
   - Â¿Es Ãºtil y profesional?

Responde en formato:
STATUS: [APPROVED o NEEDS_IMPROVEMENT]
SCORE: [0-100]
ISSUES: [lista]
SUGGESTIONS: [lista]
FEEDBACK: [explicaciÃ³n especÃ­fica]
```

### DecisiÃ³n: SIEMPRE Mejorar

**Por quÃ© SIEMPRE se ejecuta la segunda iteraciÃ³n:**
- âœ… Incluso respuestas "APPROVED" pueden mejorarse
- âœ… El revisor siempre proporciona sugerencias constructivas
- âœ… La segunda iteraciÃ³n puede agregar mÃ¡s contexto
- âœ… Garantiza mÃ¡xima calidad en todas las respuestas
- âœ… El usuario solicitÃ³ este comportamiento explÃ­citamente

---

## ğŸ“Š Ejemplos Reales

### Ejemplo 1: Consulta Simple â†’ Review â†’ Mejora

**Input:** "Dame licitaciones de IT"

**IteraciÃ³n 1:**
- Tools: `search_tenders(query="IT", limit=10)`
- Respuesta: "He encontrado 10 licitaciones de IT..."
- Formato: Lista numerada 1, 2, 3...

**Review:**
- Status: NEEDS_IMPROVEMENT
- Score: 65/100
- Issue: "Usa lista numerada en vez de headers ##"
- Suggestion: "Usar ## para cada licitaciÃ³n"

**IteraciÃ³n 2 (mejora):**
- Lee feedback
- No usa tools adicionales
- Reformatea con ## para cada licitaciÃ³n
- Respuesta mejorada con formato correcto

**Resultado:** Mismos datos, mejor formato

---

### Ejemplo 2: Consulta Compleja â†’ Review â†’ BÃºsqueda Adicional

**Input:** "CuÃ¡les son las mejores licitaciones para mi empresa?"

**IteraciÃ³n 1:**
- Tools: `search_tenders(query="licitaciones", limit=10)`
- Respuesta: "EncontrÃ© 10 licitaciones..."
- Issue: No usa perfil de empresa

**Review:**
- Status: NEEDS_IMPROVEMENT
- Score: 70/100
- Issue: "Falta anÃ¡lisis personalizado"
- Suggestion: "Usar get_company_info() para contexto"

**IteraciÃ³n 2 (mejora):**
- Tools: `get_company_info()`, `get_tender_details(id1)`, `get_tender_details(id2)`
- Genera anÃ¡lisis de match basado en perfil
- Explica por quÃ© cada licitaciÃ³n es adecuada
- Respuesta con anÃ¡lisis completo

**Resultado:** MÃ¡s documentos, mejor anÃ¡lisis

---

### Ejemplo 3: Respuesta Correcta â†’ Review â†’ Refinamiento

**Input:** "Presupuesto de licitaciÃ³n 00668461-2025"

**IteraciÃ³n 1:**
- Tools: `get_tender_details(tender_id="00668461-2025")`
- Respuesta: "El presupuesto es 961,200 EUR"
- Formato: Correcto

**Review:**
- Status: APPROVED
- Score: 85/100
- Issue: Ninguno
- Suggestion: "Agregar contexto (comprador, plazo)"

**IteraciÃ³n 2 (mejora):**
- No usa tools adicionales (ya tiene los datos)
- Agrega informaciÃ³n de contexto
- Respuesta: "El presupuesto es 961,200 EUR. Comprador: FundaciÃ³n Estatal. Plazo: 45 dÃ­as restantes."

**Resultado:** Respuesta mÃ¡s completa

---

## âš™ï¸ ConfiguraciÃ³n

**Variables relevantes en .env:**
```bash
# LLM Settings
LLM_PROVIDER=openai
LLM_TEMPERATURE=0.3

# Iteraciones
MAX_ITERATIONS=15

# Review (siempre activo, no configurable)
```

**User model:**
```python
llm_provider = 'openai'
openai_model = 'gpt-4o-mini'
llm_api_key = 'sk-...'
```

---

## ğŸ“Š MÃ©tricas

### Tokens Consumidos (ejemplo)

| Etapa | Tokens In | Tokens Out | Total |
|-------|-----------|------------|-------|
| IteraciÃ³n 1 (3 ciclos) | 400 | 250 | 650 |
| Review | 150 | 100 | 250 |
| IteraciÃ³n 2 (2 ciclos) | 350 | 200 | 550 |
| **TOTAL** | **900** | **550** | **1450** |

### Latencia (ejemplo con OpenAI)

| Etapa | Tiempo |
|-------|--------|
| IteraciÃ³n 1 | 1.2s |
| Review | 0.4s |
| IteraciÃ³n 2 | 0.9s |
| Merge + BD | 0.1s |
| **TOTAL** | **2.6s** |

---

## ğŸ”— Referencias

- **Arquitectura**: [ARCHITECTURE.md](ARCHITECTURE.md)
- **Tools**: [TOOLS_REFERENCE.md](TOOLS_REFERENCE.md)
- **ConfiguraciÃ³n**: [CONFIGURACION_AGENTE.md](CONFIGURACION_AGENTE.md)

---

**VersiÃ³n**: 3.7.0
**Ãšltima actualizaciÃ³n**: 2025-01-19
**Feature destacada**: Review Loop automÃ¡tico SIEMPRE activo

**ğŸ¤– Generated with [Claude Code](https://claude.com/claude-code)**

**Co-Authored-By: Claude <noreply@anthropic.com>**
